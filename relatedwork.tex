\chapter{Related Work}\label{relatedwork}
% In this chapter you demonstrate that you are sufficiently aware of the
% state-of-art knowledge of the problem domain that you have investigated as
% well as demonstrating that you have found a \emph{new} solution / approach / method.
In this section we will discuss some of the previous work that has been done to detect DGA domains. There are multiple approaches to research DGA domains.\\\\
One of the first approaches to detect DGA domains is by using unsupervised learning. Chang and Lin \cite{Chang_Lin} propose a dynamic way to detect botnets DNS traffic monitoring. First, the known benign and malicious domain names are filtered in the DNS traffic.  Afterwards, the Chinese-Whispers algorithm is applied to the remaining domains to cluster them according to the similarity of the query behaviour. 
Zhou et al. \cite{Zhou2013DGABasedBD} use a passive DNS dataset to record the information of domain access, consisting of 18 features, to detect Fast-Flux domains using random forest algorithm. 
Knowing that not resolved DGA domains result in NXDomain responses, Antonakakis et al. \cite{Antonakakis} classify and cluster the domains with Hidden Markov Models (HMM). However, because the clustering strategy rel on domain names' structural and lexical features, it is limited to DGA-based C \& C only.\\\\ 
Woodbridge et al. \cite{Woodbridge} is the first to utilize supervised deep learning for DGA detection. A simple implementation of an LSTM is used for non-specific DGA analysis. They show that their LSTM network outperforms unsupervised learning methods such as character-level HMM and random forest models. Nonetheless, their LSTM model does not have a high score on suppobox or matsnu, the dictionary DGA families. Their research has inspired other research to use supervised learning methods to better identify DGA domains. 
In a different angle Anderson et al. \cite{Anderson} use a generative Adversarial Network (GAN) to investigate if the adversarial learning technique is able to deceive DGA detection.
\pagebreak
\\Tran et al. \cite{TRAN20182401} present a novel LSTM.MI algorithm that combines both binary and multiclass classification models to improve the cost-effectiveness of the LSTM. They demonstrate that the LSTM.MI algorithm provides an improvement of at least 7\% compared to the original LSTM.
Chen et al. \cite{Chen} propose a LSTM Property and Quantity Dependent Optimization (LSTM.PQDO) that dynamically optimizes the resampling proportion of the original number and characteristics of the samples. This research results in a better performance compared to earlier models by overcoming the difficulties of unbalanced datasets. Another research done by Lison et al. \cite{Lison} alter the structure of the LSTM to a bi-directional LSTM layer. The enhancement of the bi-directional LSTM layer results in a $F_1$ score of 0.971.\\\\
Koh et al. \cite{Koh} are one of the first that utilized deep learning to train their model. They classify domains based on word-level information by combining pre-trained context-sensitive word embedding with a classifier. The LSTM is trained both on single-DGA and multiple-DGA data. The model outperforms existing techniques on wordlist-based DGA.  
Highnam et al. \cite{Highnam} research pick up on Koh et al \cite{Koh} work. By systematically evaluating deep learning, a novel hybrid neural network, called the Bilbo the fibagginfi model, is created that consists of a model which uses a convolutional neural network (CNN) and a LSTM network in parallel. 
This CNN+LSTM combo network is the most consistent in performance in terms of AUC, $F_1$ score and accuracy compared to previous work. 
\section{Contribution}
This thesis will further contribute to detecting DGA domains using deep learning. We propose a novel approach to detecting DGA domains, by utilizing the newly developed transformer models to pre-train our model that bidirectionally classifies context-sensitive word embedding of DGA domains. We use an alternative distilled version of the Bidirectional Encoder Representations from Transformers model (BERT), called DistilBERT \cite{Sanh2019DistilBERTAD}. We are one of the first to have used the BERT model to detect and classify DGA domains.