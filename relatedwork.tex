\chapter{Related Work}\label{relatedwork}
% In this chapter you demonstrate that you are sufficiently aware of the
% state-of-art knowledge of the problem domain that you have investigated as
% well as demonstrating that you have found a \emph{new} solution / approach / method.
In this chapter we will discuss some of the previous work that has been done to detect DGA domains. There are multiple approaches to research on DGA domains.\\\\
One of the first approach to detect DGA domains is by using unsupervised learning. Chang and Lin \cite{Chang_Lin} propose a dynamic way to detect botnets DNS traffic monitoring. First the known benign and malicious domain names are filtered in the DNS traffic.  After which the Chinese-Whispers algorithm is applied to the remaining domains to cluster them according to the similarity of the query behaviour. 
Zhou et al. \cite{Zhou2013DGABasedBD} use a passive DNS dataset to record the information of domain access, consisting of 18 features, to detect Fast-Flux domains using random forest algorithm. 
With the insight that not resolved DGA domains result in NXDomain responses, Antonakakis et al. \cite{Antonakakis} classify and cluster the domains with Hidden Markov Models (HMM). Although, because the clustering strategy relies on domain names' structural and lexical features, it is limited to DGA-based C \& C only.\\\\ 
Woodbridge et al. \cite{Woodbridge} is the first to utilize supervised deep learning for DGA detection. A simple implementation of an LSTM is used for nonspecific DGA analysis. They show that their LSTM network outperforms unsupervised learning methods such as character-level HMM and random forest models. However, their LSTM model does not have a high score on suppobox or matsnu, the dictionary DGA families. Their research have given other researchers interest in using supervised learning methods to better identifying DGA domains. 
In a different angle Anderson et al. \cite{Anderson} uses a generative Adversarial Network (GAN) to investigate if the adversarial learning techniques is able to deceive DGA detection.
\pagebreak
\\Tran et al. \cite{TRAN20182401} presents a novel LSTM.MI algorithm that combined both binary and multiclass classification models to improve the cost-effectiveness of the LSTM. They demonstrate that the LSTM.MI algorithm provides an improvement of at least 7 \% compared to the original LSTM.
Chen et al. \cite{Chen} proposes a LSTM Property and Quantity Dependent Optimization (LSTM.PQDO) that dynamically optimizes the resampling proportion of the original number and characteristics of the samples. This research results in a better performance compared to earlier models by overcoming the difficulties of unbalanced datasets. Another research done by Lison et al. \cite{Lison} alters the structure of the LSTM to a bi-directional LSTM layer. The enhancement of the bi-directional LSTM layer results in a $F_1$ score of 0.971.\\\\
Koh et al. \cite{Koh} are one of the first that utilized deep learning to train their model. They classify domains based on word-level information by combining pre-trained context-sensitive word emberdding with a classifier. The LSTM is trained both on single-DGA and multiple-DGA data. The model outperforms existing techniques on wordlist-based DGA.  
Highnam et al. \cite{Highnam} research picks up on Koh et al \cite{Koh} work. By systematically evaluating deep learning, a novel hybrid neural network, called the Bilbo the fibagginfi model, is created that consists of a model which uses a convolutional neural network (CNN) and a LSTM network in parallel. 
This CNN+LTSM combo network is the most consistant in performance in terms of AUC, $F_1$ score and accuracy compared to previous work. 
\section{Contribution}
This thesis will furthur contribute to detecting DGA domains using deep learning. We propose a novel approach to detecting DGA domains, by utilizing the newly developed transformer models to pre-train our model that bidirectionally classifies context-senstive word embedding of DGA domains. We use an alternative distilled version of the Bidirectional Encoder Representations from Transformers model (BERT), called DistilBERT \cite{Sanh2019DistilBERTAD}. It is a smaller, faster and lighter BERT model, by reducing a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. We are one of the first that have used the BERT model to detect and classify DGA domains.