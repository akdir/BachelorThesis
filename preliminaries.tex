\chapter{Preliminaries}\label{preliminaries}
This section will describe malware, the different types and how it utilizes DGA perform malicious act. It will also explain the basics of machine learning and different types of neural networks that are necessary for this research paper to know. \\\\  
Malware is a blending of two words, malicious and software, where it clearly defines the functionality of it, namely a software that is malicious in nature. Malware can have multiple purposes. Cybercriminals typically use it to extract data from the victims computer to leverage against them for financial gain. This data can range from financial data, sensitive personal data: such as healthcare records, personal emails, passwords, etc. The possibilities of the information that can be compromised are endless.\\\\ 
The most common ways victims receive malware is through the internet and mail.Malware can penetrate a victim's computer in different ways, such as: surfing to hacked websites, viewing malicious ads on websites, download infected files and install malicious programs or apps. When a malware has infected the computer system of a victim, it can come in many forms, such as Ransomware, Spyware, Trojans, Worms, etc.

\section{Botnets}
A compromised machine that is infected by malware can end up a network of infected machines (botnets). This machine is a bot in that network that receives and responds to commands from the command \& control server. The C \& C server is controlled by and receives commands by a human controller called a botmaster. The botmaster conceals itself by employing
a number of proxy machines, called the stepping stones, between it and the C \& C server. The life cycle of a botnet can be divided into four phases. For this research only the first two phases are important. The first phase is when the machine (bot) receives the malware and executes the binary.
After the machine is infected, this machine (bot) tries to contact the C \& C server to announce its presence and contact with it. This establishment phase is called Rallying. There are two ways that the bot can contact with the C \& C server. The first way that the bot knows the IP address of the C \& C server. This IP address can be hardcoded into the binary, which can be exposed by reverse engineering the binary. The IP address can also be seeded, where the bot is provided by a list of peers, this list can be hidden in Windows registries.  The second way is that the bot knows the domain name of the C \& C server.  The domain name can be hardcoded into the bot binary, where it can resolve to different IP addresses.  Reverse engineering this binary may expose
the domain name, which can then be blacklisted.

\section{Domain Generated Algorithm}
The domain name can also be generated, then the bot dynamically contacts the C \& C server using DGA (Domain name Generation Algorithm). The essence of this algorithm is that it creates a set of random strings. The bot attempts to resolve the generated domain names by sending DNS queries until one of the domains resolves to the C \& C server IP Address. The domains that do not resolve will result in Non-Existent Domain (NXDomain) responses.\\\\ 
The domain names that are generated by the DGA are also known as Algorithmically Generated Domains (AGD). The DGA uses a seed to serve as a shared secret between the botmaster and the bot. There are two types of seeds: static seed and dynamic seed. The seed is required to calculate the AGDs. The DGA takes the seed value as input to generate pseudo-random strings and append algorithmically TLD (Top Level domains) such as \textit{.nl, .com, .org, .edu}. The static seed can be dictionary of words, random strings that are concatenated, numbers or any other value that the botmaster can come up with. Dynamic seeds are dynamic, it changes with time. Dynamic seeds can be currency exchange rate, daily trending twitter hashtag, weather temperature and current date and time.  The static and dynamic seed elements are then stitched together to generate a pseudo-random string.\\\\ 
The botmaster uses the DGA to generate a large number of domain names for the C \& C server.  The constant change of domain names for the C \& C server using DGA is known as Domain-Fluxing.  The botmaster registers one of the DGA created domain names for the C \& C server in advance using the same algorithm of the DGA.  When the bot receives the malware, the malware queries to the pre-registered domain name and resolves the IP address using DNS.  The botmaster registers the domain name most of the time some hours prior to an attack and disposes of the domain names within a day.  Whenever the previous domain name that the bot connected with does not resolve anymore, it queries to the next set of generated domain names until it find one domain that works.\\\\ 
The DGA and constant domain-fluxing of the C \& C server provides agility and resilience to the infrastructure of the botmaster.  This makes it hard to predict what domain names a bot will try. Analyst will re-engineer DGA by analyzing the malware and understand how the algorithm works. It is still hard to predict what kind of seed the DGA will use on a specific time. It is also infeasible to report all the domain names that are generated. As some DGA use english dictionary as static seed values, it is hard to distinguish benign domain names from malicious ones. 

\section{Machine Learning}
Machine learning has recently been an attractive tool to be used in security. One way to combat DGA is to use machine learning to find the structure of the generated domains. The machine learning methods can be either supervised or unsupervised. Unsupervised learning uses algorithms to analyze and cluster data, in this case the domains. These algorithms discover hidden patterns or data groupings, without a need for a human intervention. There are three ways to approach unsupervised learning: clustering, association and dimensionality reduction. The domains are divided into clusters to find statistical attributes for each group. To produce a cluster with good generalization capabilities, it can take a lot of time and  effort \cite{Unsupervised}. Supervised learning does not rely on the statistical attributes  for each group to find DGAs. Supervised learning attempts to understand and classify the input and predict the outcome accurately. The relationship is represented as a structure to predict the outputs for some specific future inputs.

\section{Neural Networks}
Artificial Neural Networks are artificial systems that were inspired by the biological counterpart. The systems learn in a supervised manner to perform tasks by using various datasets and examples. These neural networks are composed of node layers, containing one or more input layers, hidden layers and output layers. Each node is connected to another node and has an associated weight w and threshold. When the threshold of a specific node is above a certain threshold value, then that node is activated, otherwise no data is passed along to the next layer of the network. \\\\ 
The network uses training data to learn and improve the accuracy of the network. This is usually done by backpropagation. Backpropagation is a supervised learning algorithm that computes the difference between the model output and the actual output using gradient descent and the chain rule . It checks if the error is minimized and update the weights and biases accordingly. It repeats the process until the error becomes minimum \cite{Gradient_Descent}.

\subsection{Recurrent Neural Networks}
Recurrent neural networks or RNN, are a type of neural network that uses the output from the previous step and fed that as input in the current step, while in traditional Neural networks the network assumes that the inputs and outputs are independent of each other. The cost function or error can be calculated at any time $t$. At any given time $t$, the current input is a combination of $x_t$ and $x_{t-1}$. This makes the neural network recurrent, it has feedback loops at each iteration of the hidden layer. One thing Recurrent Neural Networks are used for are Sequence Modeling. Sequence Modeling is the task to predict about future outcomes.\\

\includegraphics[width=12cm]{RNN}

\section{Activation Functions}
The activation functions in a neural network are functions that determine the output of a neural network. It maps it to a specific value. The function receives the calculated weighted sum of the inputs and the added bias and then decides if this passes through or not. \\\\
There are two types of activation functions: Linear Activation function and Non-linear Activation Functions.
The linear activation functions are functions that does not change the weighted sum of the input, but instead returns the value directly. For multiple layer networks we need non-linear activation function.

\subsection{ReLU}
ReLU, or Rectified Linear Unit, is linear when the input is positive and 0 when the input is negative. The range of ReLu is [0, inf). The benefit of ReLu is that there is a reduced likelihood of the gradient to vanish(see subsection Vanish Gradient Problem), as the gradient is constant. This constant gradient results in faster learning of the network. Another benefit is the sparsity, as the network has more units in a layer, other activation functions will be processed to describe the output of that network. When the calculated sum that goes into the activation of ReLu is negative, it yields 0. This means there is fewer neurons firing, which makes the network lighter.\\\\
The disadvantage is that it tends to blow up, as the range goes to infinity and there is no mechanism to constrain the output when it is positive. Another disadvantage is the problem that if too many activations in the network get below zero, than the neurons in the network will output zero. This means that the outputs die out, thus prohibiting learning. This is called the Dying ReLu problem.

\includegraphics[width=10cm]{relu}

\subsection{Sigmoid}
Sigmoid activation function is a non-linear activation function that looks like an S-shape. Any small changes in the incoming X value(the calculated sum) will cause the Y value (the output) to change significantly. The range is [0,1], so it is bounded in a range and it does not blow up. The disadvantage of the Sigmoid function is the vanishing gradients(see subsection Vanishing Gradient Problem)

\includegraphics[width=10cm]{sigmoid_function}

\subsection{Tanh}
The Tanh activation function is very similar to the Sigmoid function. The difference is that the range is [-1,1] and the gradient is stronger for tanh than sigmoid. That means the derivative are steeper. One benefit of tanh over sigmoid is that it avoids bias in the gradients \cite{Tan_h}.

\includegraphics[width=10cm]{tanh_function}

\subsection{Vanishing Gradient problem}
The vanishing gradient problem can be encountered when gradient-based learning methods and backpropagation is used. As there are more layers using activation function are added to the neural network, the gradients of these loss functions approaches zero. The gradient will be vanishingly small, which in turn prevents the current weight from changing its value. This can lead to the neural network to stop further training. As mentioned before, the sigmoid activation function squishes a large input space into a value between 0 and 1. The effect of this is that a large change in input, would cause a small change in the output. The derivative therefore becomes very small. The derivative would become close to zero, which causes the gradient of this layer in the network to vanish.\\\\
One network that suffers from vanishing gradient problem is a traditional Recurrent Neural Network. At the time that the cost function is calculated in a RNN using a activation function that is vulnerable to the vanishing gradient problem, When the feedback loops occurs and when the gradient is getting lower, then it is more harder for the network to update it weights. The weights of the initial value will not change effectively through the training process, which can lead to inaccuracy in the network. \\\\
The solution to this problem is to use other activation functions, such as ReLu, which doesn't cause a small derivative. Another solutions is to use residual networks \cite{DBLP:journals/corr/HeZRS15} or an RNN architecture that is resistant to this problem. One of these architectures is the Long short-term memory (LSTM) network. 

\subsection{LSTM}
Long short-term memory (LSTM) is a specialized RNN, that is capable of learning in long-term dependencies. It is designed to remember information for long periods of time. While standard RNN are vulnerable to vanishing gradient problem, it is more difficult for standard RNNs to connect information between far apart hidden layers. As that gap grows, the RNNs becomes unable to learn that information. This is because of the vanishing gradient problem. Thus, it is resistant to the vanishing gradient problem. In a traditional recurrent neural network, the form is different
\subsection{The core of LSTM}
\subsection{Transformers}
\subsection{BERT}
BERT (Bidirectional Encoder Representations from Transformers) is a machine learning technique based on transformers. It is used for pre-training natural language processing (NLP). The main innovation of this technique is applying bidirectional training on a Transformer model. In contrast, other efforts looked at it in a single direction, from left to right or right to left. \\\\
BERT only uses the encoder mechanism to generate a language model. The encoder of BERT reads the entire sequence of words at once. That ensures that the model learns the context of a word from all its surroundings, thus making it bidirectional. 
BERT are pretrained on two tasks: language modelling (LM) and next sequence prediction (NSP). It uses Masked LM (MLM) for pretraining language modelling. This is done by replacing 15\% of any sequence with a mask token. The model tries to predict the original value of the masked words, using the context provided by the other non-masked words in a sequence. 

The next task of the training process, BERT uses next sentence prediction to understand the relationship between two sentences. The model receives as input pairs sentences and it learns to predict the second sentence is the next sentence in the original document. BERT separates sentences with a special [SEP] token. Then the model is fed with two input sentences at a time. During training, 50\% of the time the second sentence is the subsequent sentence in the original document, while in the other 50\% of the time it is a random sentence from the full corpus. The assumption being that the random sentence will be disconnected from the first sentence. 

\includegraphics[width=10cm]{BERT}